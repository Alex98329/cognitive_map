{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.28.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5Xva7md4vgn",
        "outputId": "bdf0a206-4a71-4e19-85fe-ead28c403d6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.28.0\n",
            "  Downloading openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28.0) (4.67.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28.0) (3.11.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.0) (2024.12.14)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->openai==0.28.0) (4.12.2)\n",
            "Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.57.4\n",
            "    Uninstalling openai-1.57.4:\n",
            "      Successfully uninstalled openai-1.57.4\n",
            "Successfully installed openai-0.28.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/raphael-sch/VELMA.git\n",
        "%cd VELMA"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x96cYuxoYTZI",
        "outputId": "dd630c15-e3dd-44a9-ec92-65a3d9f31545"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'VELMA'...\n",
            "remote: Enumerating objects: 137, done.\u001b[K\n",
            "remote: Counting objects: 100% (33/33), done.\u001b[K\n",
            "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
            "remote: Total 137 (delta 14), reused 24 (delta 10), pack-reused 104 (from 1)\u001b[K\n",
            "Receiving objects: 100% (137/137), 101.75 MiB | 6.66 MiB/s, done.\n",
            "Resolving deltas: 100% (37/37), done.\n",
            "Updating files: 100% (78/78), done.\n",
            "/content/VELMA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import pandas as pd\n",
        "import openai\n",
        "import base64\n",
        "import requests\n",
        "import io\n",
        "import math\n",
        "import html\n",
        "import re\n",
        "import cv2\n",
        "import numpy as np\n",
        "import imageio\n",
        "import os\n",
        "import csv\n",
        "import json"
      ],
      "metadata": {
        "id": "vxBfueDx7QIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "aca se guardan set de coordenadas, distancias y ruta a la carpeta donde estan las imagenes e instancia"
      ],
      "metadata": {
        "id": "9ldKCKBH7ONh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "borrar lo que tenga coordenadas de los cognitive maps json, contar cuantas rutas hay y el orden"
      ],
      "metadata": {
        "id": "kZ5ayoYP7j1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate heading between two points\n",
        "def calculate_heading(lat1, lon1, lat2, lon2):\n",
        "    \"\"\"Calculate the heading between two geographic points.\"\"\"\n",
        "    lat1 = math.radians(lat1)\n",
        "    lon1 = math.radians(lon1)\n",
        "    lat2 = math.radians(lat2)\n",
        "    lon2 = math.radians(lon2)\n",
        "\n",
        "    d_lon = lon2 - lon1\n",
        "    x = math.sin(d_lon) * math.cos(lat2)\n",
        "    y = math.cos(lat1) * math.sin(lat2) - (math.sin(lat1) * math.cos(lat2) * math.cos(d_lon))\n",
        "\n",
        "    initial_bearing = math.atan2(x, y)\n",
        "    initial_bearing = math.degrees(initial_bearing)\n",
        "    compass_bearing = (initial_bearing + 360) % 360\n",
        "\n",
        "    return compass_bearing"
      ],
      "metadata": {
        "id": "uW3uXyHr65uo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def location_heading_func(index, threshold, my_panoid):\n",
        "    data = []\n",
        "    for i, pan in enumerate(my_panoid[:-1]):  # Loop through all but the last pano\n",
        "        current_location = nodes[nodes['Panoid'] == pan]\n",
        "        next_location = nodes[nodes['Panoid'] == my_panoid[i + 1]]  # Use the next pano directly\n",
        "\n",
        "        # Check if the current and next locations are found\n",
        "        if not current_location.empty and not next_location.empty:\n",
        "            current_point = current_location['location'].values[0]\n",
        "            next_point = next_location['location'].values[0]\n",
        "\n",
        "            # If the points are formatted as strings (e.g., \"lat, lon\"), split them\n",
        "            if isinstance(current_point, str):\n",
        "                current_point = [float(coord.strip()) for coord in current_point.split(',')]\n",
        "            if isinstance(next_point, str):\n",
        "                next_point = [float(coord.strip()) for coord in next_point.split(',')]\n",
        "\n",
        "            # Calculate the heading\n",
        "            t_heading = calculate_heading(current_point[0], current_point[1], next_point[0], next_point[1])\n",
        "            # Append data to the list\n",
        "            data.append({\n",
        "                'Current Point': current_point,\n",
        "                'Current Heading': t_heading\n",
        "                })\n",
        "\n",
        "    location_heading = pd.DataFrame(data)\n",
        "\n",
        "    # List to store exceeding threshold data\n",
        "    exceeding_threshold_data = []\n",
        "    exceeding_threshold_data.append(location_heading.iloc[0].to_dict())\n",
        "    # Iterate through the DataFrame to calculate heading differences\n",
        "    for i in range(len(location_heading) - 1):\n",
        "        current_heading = location_heading[\"Current Heading\"].iloc[i] % 360\n",
        "        next_heading = location_heading[\"Current Heading\"].iloc[i + 1] % 360\n",
        "\n",
        "        # Calculate the absolute difference in headings\n",
        "        heading_difference = abs(current_heading - next_heading)\n",
        "\n",
        "        # Normalize the heading difference to be within 0-180 degrees\n",
        "        if heading_difference > 180:\n",
        "            heading_difference = 360 - heading_difference\n",
        "\n",
        "        # Check if the heading difference exceeds the threshold\n",
        "        if heading_difference >= heading_threshold:\n",
        "            current_point = location_heading[\"Current Point\"].iloc[i]\n",
        "            next_point = location_heading[\"Current Point\"].iloc[i + 1]\n",
        "            exceeding_threshold_data.append({\n",
        "                'Current Point': current_point,\n",
        "                'Current Heading': current_heading\n",
        "            })\n",
        "    exceeding_threshold_data.append({\n",
        "        'Current Point': location_heading[\"Current Point\"].iloc[-1],  # Last row's Current Point\n",
        "        'Current Heading': location_heading[\"Current Heading\"].iloc[-2]  # Second-to-last row's Current Heading\n",
        "    })\n",
        "\n",
        "    exceeding_threshold_df = pd.DataFrame(exceeding_threshold_data)\n",
        "    location = exceeding_threshold_df['Current Point']\n",
        "    heading = exceeding_threshold_df['Current Heading']\n",
        "\n",
        "    return (location, heading)"
      ],
      "metadata": {
        "id": "gdtTM6bXXiVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to your JSON file\n",
        "json_file = '/content/VELMA/datasets/map2seq_seen/data/train.json'\n",
        "\n",
        "routes = []\n",
        "\n",
        "# Load the JSON file and split the content by newlines\n",
        "with open(json_file, 'r') as file:\n",
        "    content = file.read()  # Read the file content\n",
        "\n",
        "    # Split by lines (assuming each line is a separate JSON object)\n",
        "    json_objects = content.strip().split('\\n')\n",
        "\n",
        "    # Parse each JSON object separately\n",
        "    for obj in json_objects:\n",
        "        try:\n",
        "            data = json.loads(obj)  # Parse each JSON object\n",
        "            routes.append(data)    # Append the parsed data to the list\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"Error decoding JSON object: {e}\")\n",
        "\n",
        "# Convert the list of dictionaries to a DataFrame\n",
        "df = pd.DataFrame(routes)\n",
        "\n",
        "# Print the DataFrame to check if everything is loaded correctly\n",
        "print(df)\n",
        "\n",
        "# If you need to access the length of 'id' for each row in the DataFrame:\n",
        "df['id_length'] = df['id'].astype(str).apply(len)\n",
        "\n",
        "# Print the DataFrame with the new column\n",
        "unique_id_count = df['instructions_id'].nunique()\n",
        "print(f\"Number of unique IDs: {unique_id_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzwFOnMDXlHT",
        "outputId": "50e85415-ece7-4fb4-dd98-7404909740e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        id  instructions_id  \\\n",
            "0     3890             6199   \n",
            "1      762             2386   \n",
            "2      566             2170   \n",
            "3     6621            11773   \n",
            "4     3206             5181   \n",
            "...    ...              ...   \n",
            "6067  6941            11942   \n",
            "6068  4245             6023   \n",
            "6069  3130             5026   \n",
            "6070  4233             5997   \n",
            "6071  4241             6014   \n",
            "\n",
            "                                          route_panoids  \\\n",
            "0     [pC21-8fg49EaRxpvXF2M_A, lXPHjkUOnMF4c4q_laK3t...   \n",
            "1     [n11iwpgfziGtTxMiEfe2tA, fC-4WDDSswU95ul7VjqtK...   \n",
            "2     [rYkV6xH5B9qBBrs53gDDsw, mzcnwsvM2XVAohfNtj863...   \n",
            "3     [LxkBjn_U-VbZDPgCEoY4TQ, ai-OcfBxtL1RmQKe1s-9z...   \n",
            "4     [eXCmBxpFyJvJCYKF9l1k9A, 5UmtlXcSDEXdfCFXcHPmz...   \n",
            "...                                                 ...   \n",
            "6067  [RzmE_OnMqrFKr0nIMiOC9g, ouvW2TDorMt6vuaA5b4gY...   \n",
            "6068  [XlntDT1inp5L7EzgNhSbEg, dqTddv8tYDc2F0X9v5evj...   \n",
            "6069  [gG5kygeBGUYtCZuSweJChw, bBQ-MVk4cgtBbB3lp8vzS...   \n",
            "6070  [7bXzTG88LsoAY5slrW3-Kw, ix4zhtXzcPDC76jSuAPQi...   \n",
            "6071  [Cza-CqqBtDK5nTHeFM9Xug, OjMflp8gcs_uxZ26U6S_N...   \n",
            "\n",
            "                                        navigation_text  start_heading  \n",
            "0     Go straight until merging onto the main road a...            265  \n",
            "1     Go straight to first intersection and make a r...            123  \n",
            "2     Turn right at the lights ahead of you. Pass th...              6  \n",
            "3     Turn left at the first set of lights. Pass Amo...            299  \n",
            "4     Take the sharp right at the first intersection...              1  \n",
            "...                                                 ...            ...  \n",
            "6067  Head to the end of the block but you will veer...            299  \n",
            "6068  Go to the stoplight and take a left. Happy Soc...            302  \n",
            "6069  Head through the first block and make a left a...            260  \n",
            "6070  Walk straight ahead through two lights. After ...            197  \n",
            "6071  Head to the first light and make a left. At th...             22  \n",
            "\n",
            "[6072 rows x 5 columns]\n",
            "Number of unique IDs: 6072\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to your text file\n",
        "file_path = '/content/VELMA/datasets/map2seq_seen/graph/nodes.txt'\n",
        "\n",
        "# Define the column names\n",
        "column_names = ['Panoid', 'Heading', 'Lat', 'Lon', 'type']\n",
        "\n",
        "# Read the text file into a DataFrame without a header, and assign column names\n",
        "nodes = pd.read_csv(file_path, header=None, names=column_names)\n",
        "nodes['location'] = nodes['Lat'].astype(str) + ', ' + nodes['Lon'].astype(str)\n",
        "\n",
        "nodes = nodes.drop(columns=['Lat','Lon'])\n",
        "nodes.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "nEvX5AFyXks5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert all non-serializable types to standard Python types\n",
        "def convert_to_standard_types(data):\n",
        "    if isinstance(data, dict):\n",
        "        return {key: convert_to_standard_types(value) for key, value in data.items()}\n",
        "    elif isinstance(data, list):\n",
        "        return [convert_to_standard_types(item) for item in data]\n",
        "    elif isinstance(data, np.integer):  # Handle numpy integer\n",
        "        return int(data)\n",
        "    elif isinstance(data, np.floating):  # Handle numpy float\n",
        "        return float(data)\n",
        "    else:\n",
        "        return data"
      ],
      "metadata": {
        "id": "KCQGpaW5cb7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate the Haversine distance between two points\n",
        "def haversine(lat1, lon1, lat2, lon2):\n",
        "    R = 6371000  # Radius of the Earth in meters\n",
        "    dlat = np.radians(lat2 - lat1)\n",
        "    dlon = np.radians(lon2 - lon1)\n",
        "    a = (np.sin(dlat / 2) ** 2 +\n",
        "         np.cos(np.radians(lat1)) * np.cos(np.radians(lat2)) * np.sin(dlon / 2) ** 2)\n",
        "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
        "    distance = R * c  # Distance in meters\n",
        "    return distance\n",
        "\n",
        "# Function to interpolate points between two latitude-longitude coordinates\n",
        "def interpolate_points(lat1, lon1, lat2, lon2, num_points):\n",
        "    lat_points = np.linspace(lat1, lat2, num=num_points + 2)[1:-1]\n",
        "    lon_points = np.linspace(lon1, lon2, num=num_points + 2)[1:-1]\n",
        "    return list(zip(lat_points, lon_points))\n",
        "\n",
        "# Function to convert numpy types to standard Python types\n",
        "def convert_to_standard_types(obj):\n",
        "    if isinstance(obj, dict):\n",
        "        return {k: convert_to_standard_types(v) for k, v in obj.items()}\n",
        "    elif isinstance(obj, list):\n",
        "        return [convert_to_standard_types(i) for i in obj]\n",
        "    elif isinstance(obj, np.integer):\n",
        "        return int(obj)\n",
        "    elif isinstance(obj, np.floating):\n",
        "        return float(obj)\n",
        "    elif isinstance(obj, np.ndarray):\n",
        "        return obj.tolist()\n",
        "    else:\n",
        "        return obj"
      ],
      "metadata": {
        "id": "m4bGfUpXfbP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterator\n",
        "#fixed =[2849]\n",
        "\n",
        "#for i in fixed:\n",
        "for i in range(0, 6072):\n",
        "    Instance_id = df['id'][i]\n",
        "    heading_threshold = 30\n",
        "    my_panoid = df['route_panoids'][i]\n",
        "\n",
        "    (locat, headi) = location_heading_func(index=i, threshold=heading_threshold, my_panoid=my_panoid)\n",
        "    location = pd.DataFrame(locat)\n",
        "    heading_data = []\n",
        "    distance_data = []\n",
        "    location_list = []  # Store locations with interpolated points included\n",
        "    distance_data = []\n",
        "    k = 0\n",
        "    # Loop through all but the last pano\n",
        "    for j in range(len(location) - 1):\n",
        "        current_location = location.iloc[j]['Current Point']\n",
        "        next_location = location.iloc[j + 1]['Current Point']\n",
        "\n",
        "        current_point = current_location\n",
        "        next_point = next_location\n",
        "\n",
        "        # Calculate heading and distance between consecutive points\n",
        "        point_heading = calculate_heading(current_point[0], current_point[1], next_point[0], next_point[1])\n",
        "        point_distance = haversine(current_point[0], current_point[1], next_point[0], next_point[1])\n",
        "\n",
        "        if point_distance > 10 or k==0:\n",
        "            # Add the current location\n",
        "            location_list.append({\n",
        "                'lat': current_point[0],\n",
        "                'lon': current_point[1],\n",
        "                'heading': int(round(point_heading)),\n",
        "                'file_name': f\"{Instance_id}_{k}.jpg\"\n",
        "            })\n",
        "            k += 1\n",
        "\n",
        "    # Append the final point\n",
        "    location_list.append({\n",
        "        'lat': round(next_point[0], 8),\n",
        "        'lon': round(next_point[1], 8),\n",
        "        'heading': int(round(point_heading)),\n",
        "        'file_name': f\"{Instance_id}_{k}.jpg\"\n",
        "    })\n",
        "    k += 1\n",
        "    # Create dictionary for JSON output\n",
        "    my_dictionary = {\n",
        "        \"Instance_id\": Instance_id,\n",
        "        \"Locations\": location_list,\n",
        "    }\n",
        "\n",
        "    # Define the filename for the JSON file\n",
        "    filename = '/content/train_visual_environment.json'\n",
        "\n",
        "    # Check if the file exists\n",
        "    if os.path.exists(filename):\n",
        "        try:\n",
        "            with open(filename, 'r') as file:\n",
        "                content = file.read().strip()\n",
        "                accumulated_data = json.loads(content) if content else []\n",
        "        except json.JSONDecodeError:\n",
        "            print(\"Error decoding JSON. Initializing a new list.\")\n",
        "            accumulated_data = []\n",
        "    else:\n",
        "        accumulated_data = []\n",
        "\n",
        "    # Append the new dictionary to the accumulated data\n",
        "    accumulated_data.append(my_dictionary)\n",
        "\n",
        "    # Convert accumulated_data to standard types and save to JSON\n",
        "    with open(filename, 'w') as file:\n",
        "        json.dump(convert_to_standard_types(accumulated_data), file, indent=4)"
      ],
      "metadata": {
        "id": "W53J4njoY27V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from geopy.distance import geodesic\n",
        "\n",
        "# Load the JSON file\n",
        "file_path = filename\n",
        "with open(file_path, 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "def calculate_intermediate_points(location1, location2, num_points):\n",
        "    \"\"\"\n",
        "    Calculate intermediate points between two locations.\n",
        "\n",
        "    Args:\n",
        "        location1 (tuple): Coordinates (lat, lon) of the first point.\n",
        "        location2 (tuple): Coordinates (lat, lon) of the second point.\n",
        "        num_points (int): Number of intermediate points to generate.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of intermediate points [(lat, lon), ...].\n",
        "    \"\"\"\n",
        "    lat1, lon1 = location1\n",
        "    lat2, lon2 = location2\n",
        "    intermediate_points = []\n",
        "    for i in range(1, num_points + 1):\n",
        "        fraction = i / (num_points + 1)\n",
        "        lat = lat1 + fraction * (lat2 - lat1)\n",
        "        lon = lon1 + fraction * (lon2 - lon1)\n",
        "        intermediate_points.append((lat, lon))\n",
        "    return intermediate_points\n",
        "\n",
        "# Process each instance\n",
        "updated_data = []\n",
        "for instance in data:\n",
        "    instance_id = instance[\"Instance_id\"]\n",
        "    locations = instance[\"Locations\"]\n",
        "    updated_locations = []\n",
        "\n",
        "    for i in range(len(locations) - 1):\n",
        "        loc1 = locations[i]\n",
        "        loc2 = locations[i + 1]\n",
        "\n",
        "        # Calculate the distance between consecutive points\n",
        "        coord1 = (loc1[\"lat\"], loc1[\"lon\"])\n",
        "        coord2 = (loc2[\"lat\"], loc2[\"lon\"])\n",
        "        distance = geodesic(coord1, coord2).meters\n",
        "\n",
        "        # Add the current location\n",
        "        updated_locations.append(loc1)\n",
        "\n",
        "        # Determine the number of intermediate points based on distance\n",
        "        if 100 <= distance < 150:\n",
        "            num_points = 1\n",
        "        elif 150 <= distance < 200:\n",
        "            num_points = 2\n",
        "        elif distance >= 200:\n",
        "            num_points = 3\n",
        "        else:\n",
        "            num_points = 0\n",
        "\n",
        "        # Generate and add intermediate points\n",
        "        if num_points > 0:\n",
        "            intermediate_points = calculate_intermediate_points(coord1, coord2, num_points)\n",
        "            heading = loc1[\"heading\"]\n",
        "            for idx, (lat, lon) in enumerate(intermediate_points, start=1):\n",
        "                intermediate_location = {\n",
        "                    \"lat\": round(lat,7),\n",
        "                    \"lon\": round(lon,7),\n",
        "                    \"heading\": heading,  # Use the heading from the first point\n",
        "                    \"file_name\": f\"{instance_id}_intermediate_{i}_{idx}.jpg\"\n",
        "                }\n",
        "                updated_locations.append(intermediate_location)\n",
        "\n",
        "    # Add the last location of the instance\n",
        "    updated_locations.append(locations[-1])\n",
        "    updated_data.append({\n",
        "        \"Instance_id\": instance_id,\n",
        "        \"Locations\": updated_locations\n",
        "    })\n",
        "\n",
        "# Save the updated data\n",
        "output_path = filename\n",
        "with open(output_path, 'w') as output_file:\n",
        "    json.dump(updated_data, output_file, indent=4)\n",
        "\n",
        "output_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "9942DcsZbvX3",
        "outputId": "b7cb3bad-a9f2-4a69-8008-f0e5115d70aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/train_visual_environment.json'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove \"file_name\" from all locations in the updated data\n",
        "for instance in updated_data:\n",
        "    for location in instance[\"Locations\"]:\n",
        "        location.pop(\"file_name\", None)  # Remove the \"file_name\" key if it exists\n",
        "\n",
        "# Save the updated data without \"file_name\"\n",
        "output_path_no_filename = filename\n",
        "with open(output_path_no_filename, 'w') as output_file:\n",
        "    json.dump(updated_data, output_file, indent=4)\n",
        "\n",
        "output_path_no_filename\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "Ur8oOjynbwvG",
        "outputId": "b89e0494-e302-4148-ba4e-40a637d9ec51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/train_visual_environment.json'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "add relationships"
      ],
      "metadata": {
        "id": "dTvLC158obf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load merged matched nodes\n",
        "with open('/content/final_matched_nodes.json') as matched_nodes_file:\n",
        "    matched_nodes_data = json.load(matched_nodes_file)\n",
        "\n",
        "# Load environment descriptions\n",
        "with open('/content/cleaned_environment_descriptions.json') as env_desc_file:\n",
        "    environment_description_data = json.load(env_desc_file)\n",
        "\n",
        "# Actions to consider for new nodes\n",
        "valid_actions = {\"On the left\", \"On the right\"}\n",
        "\n",
        "# Process each instance\n",
        "for instance in matched_nodes_data:\n",
        "    instance_id = instance[\"Instance_id\"]\n",
        "\n",
        "    # Find corresponding environment description\n",
        "    env_instance = next((e for e in environment_description_data if e[\"Instance_id\"] == instance_id), None)\n",
        "    if not env_instance:\n",
        "        continue\n",
        "\n",
        "    env_nodes = env_instance.get(\"Node Description\", {})\n",
        "    env_actions = env_instance.get(\"actions\", [])\n",
        "\n",
        "    # Collect all matched environment node IDs\n",
        "    matched_env_ids = {match[\"Environment_Node\"][\"ID\"] for match in instance[\"Matches\"]}\n",
        "\n",
        "    # Find new environment nodes connected via valid actions\n",
        "\n",
        "        # If the 'from' node is in matched environment nodes and the action is valid, add 'to' node\n",
        "\n",
        "    # Add new nodes to the instance\n",
        "\n",
        "\n",
        "# Save the updated matched nodes with new environment nodes\n",
        "output_file_path = \"/content/updated_matched_nodes.json\"\n",
        "with open(output_file_path, \"w\") as output_file:\n",
        "    json.dump(matched_nodes_data, output_file, indent=4)\n",
        "\n",
        "print(f\"Updated matched nodes saved to: {output_file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnA4j0VM9fKJ",
        "outputId": "e1173d4c-259d-4e29-8bb0-892bf9f96b08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated matched nodes saved to: /content/updated_matched_nodes.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load updated matched nodes (with new environment nodes)\n",
        "with open('/content/updated_matched_nodes.json') as matched_nodes_file:\n",
        "    matched_nodes_data = json.load(matched_nodes_file)\n",
        "\n",
        "# Process each instance to update Action_From references\n",
        "for instance in matched_nodes_data:\n",
        "    instance_id = instance[\"Instance_id\"]\n",
        "\n",
        "    # Step 1: Create a mapping from Environment Node ID â†’ Cognitive Node ID\n",
        "    env_to_cog_id_map = {\n",
        "        match[\"Environment_Node\"][\"ID\"]: match[\"Cognitive_Node\"][\"ID\"]\n",
        "        for match in instance[\"Matches\"]\n",
        "    }\n",
        "\n",
        "    # Step 2: Update the \"Action_From\" field in \"New_Environment_Nodes\"\n",
        "    for new_env_node in instance.get(\"New_Environment_Nodes\", []):\n",
        "        env_node = new_env_node[\"Environment_Node\"]\n",
        "        action_from = env_node[\"Action_From\"]\n",
        "\n",
        "        # Replace \"Action_From\" with its corresponding Cognitive Node ID\n",
        "        if action_from in env_to_cog_id_map:\n",
        "            env_node[\"Action_From\"] = env_to_cog_id_map[action_from]\n",
        "\n",
        "# Save the updated matched nodes\n",
        "output_file_path = \"/content/updated_nodes_id.json\"\n",
        "with open(output_file_path, \"w\") as output_file:\n",
        "    json.dump(matched_nodes_data, output_file, indent=4)\n",
        "\n",
        "print(f\"Final updated matched nodes saved to: {output_file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mM-FTvuFAdct",
        "outputId": "96683288-bb46-47ed-a553-efa79708194a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final updated matched nodes saved to: /content/updated_nodes_id.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Organice"
      ],
      "metadata": {
        "id": "T7RV03nEoffh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load cognitive map\n",
        "with open('/content/dev_graph_lmm.json') as cognitive_map_file:\n",
        "    cognitive_map_data = json.load(cognitive_map_file)\n",
        "\n",
        "# Load updated matched nodes (with new environment nodes)\n",
        "with open('/content/updated_nodes_id.json') as matched_nodes_file:\n",
        "    matched_nodes_data = json.load(matched_nodes_file)\n",
        "\n",
        "# Convert matched data to a dictionary for easy lookup\n",
        "matched_lookup = {match[\"Instance_id\"]: match for match in matched_nodes_data}\n",
        "\n",
        "# Process each cognitive instance\n",
        "updated_cognitive_map = []\n",
        "\n",
        "for cognitive_instance in cognitive_map_data:\n",
        "    instance_id = cognitive_instance[\"Instance_id\"]\n",
        "\n",
        "    # Get matched data for this instance\n",
        "    matched_data = matched_lookup.get(instance_id, {})\n",
        "\n",
        "    if not matched_data:\n",
        "        updated_cognitive_map.append(cognitive_instance)  # No changes, add as-is\n",
        "        continue\n",
        "\n",
        "    # Extract cognitive nodes and edges\n",
        "    cognitive_nodes = cognitive_instance[\"Graph\"][\"Nodes\"]\n",
        "    cognitive_edges = cognitive_instance[\"Graph\"][\"Edges\"]\n",
        "\n",
        "    # Step 1: Update existing cognitive nodes with matched environment descriptions\n",
        "    for match in matched_data.get(\"Matches\", []):\n",
        "        cog_id = match[\"Cognitive_Node\"][\"ID\"]\n",
        "        env_desc = match[\"Environment_Node\"][\"Description\"]\n",
        "        match_type = match[\"Match_Type\"]\n",
        "\n",
        "        if \"Semantic Match\" in match_type:\n",
        "            score = match_type.split(\":\")[-1].strip(\")\").strip()\n",
        "            cognitive_nodes[cog_id] += f\"; Matched Scene: {env_desc} (Semantic Match: {score})\"\n",
        "        else:\n",
        "            cognitive_nodes[cog_id] += f\"; Matched Scene: {env_desc}\"\n",
        "\n",
        "    # Step 2: Add newly discovered environment nodes as cognitive nodes\n",
        "    for new_node in matched_data.get(\"New_Environment_Nodes\", []):\n",
        "        env_id = new_node[\"Environment_Node\"][\"ID\"]\n",
        "        env_desc = new_node[\"Environment_Node\"][\"Description\"]\n",
        "        action_from = new_node[\"Environment_Node\"][\"Action_From\"]\n",
        "        action_type = new_node[\"Environment_Node\"][\"Action_Type\"]\n",
        "\n",
        "        if env_id not in cognitive_nodes:  # Avoid duplicate additions\n",
        "            cognitive_nodes[env_id] = f\"{env_desc} (New Context Node)\"\n",
        "            cognitive_edges.append({\"from\": action_from, \"to\": env_id, \"action\": action_type})\n",
        "\n",
        "    # Save updated cognitive instance\n",
        "    updated_cognitive_map.append({\n",
        "        \"Instance_id\": instance_id,\n",
        "        \"Navigation_instruction\": cognitive_instance[\"Navigation_instruction\"],\n",
        "        \"Graph\": {\n",
        "            \"Nodes\": cognitive_nodes,\n",
        "            \"Edges\": cognitive_edges\n",
        "        }\n",
        "    })\n",
        "\n",
        "# Save the updated cognitive map\n",
        "output_file_path = \"/content/organiced_cognitive_map.json\"\n",
        "with open(output_file_path, \"w\") as output_file:\n",
        "    json.dump(updated_cognitive_map, output_file, indent=4)\n",
        "\n",
        "print(f\"Updated cognitive map saved to: {output_file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqqujtmH9zyu",
        "outputId": "7840bb04-c82a-4931-81d4-bd6ccb43d6f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated cognitive map saved to: /content/organiced_cognitive_map.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load updated cognitive map\n",
        "with open('/content/organiced_cognitive_map.json') as cognitive_map_file:\n",
        "    cognitive_map_data = json.load(cognitive_map_file)\n",
        "\n",
        "# Process each instance\n",
        "for cognitive_instance in cognitive_map_data:\n",
        "    instance_id = cognitive_instance[\"Instance_id\"]\n",
        "    cognitive_nodes = cognitive_instance[\"Graph\"][\"Nodes\"]\n",
        "    cognitive_edges = cognitive_instance[\"Graph\"][\"Edges\"]\n",
        "\n",
        "    # Create a mapping of old node order to new order\n",
        "    new_node_order = {old_id: str(idx + 1) for idx, old_id in enumerate(sorted(cognitive_nodes.keys(), key=int))}\n",
        "\n",
        "    # Step 1: Rename nodes to their new ordered IDs\n",
        "    updated_nodes = {new_node_order[old_id]: desc for old_id, desc in cognitive_nodes.items()}\n",
        "\n",
        "    # Step 2: Update edge `from` and `to` references\n",
        "    updated_edges = []\n",
        "    for edge in cognitive_edges:\n",
        "        from_id = edge[\"from\"]\n",
        "        to_id = edge[\"to\"]\n",
        "\n",
        "        updated_edges.append({\n",
        "            \"from\": new_node_order.get(str(from_id), str(from_id)),  # Update 'from'\n",
        "            \"to\": new_node_order.get(str(to_id), str(to_id)),        # Update 'to'\n",
        "            \"action\": edge[\"action\"]\n",
        "        })\n",
        "\n",
        "    # Save updated cognitive instance\n",
        "    cognitive_instance[\"Graph\"][\"Nodes\"] = updated_nodes\n",
        "    cognitive_instance[\"Graph\"][\"Edges\"] = updated_edges\n",
        "\n",
        "# Save the final cognitive map with updated edge references\n",
        "output_file_path = \"/content/final_cognitive_map_sbert.json\"\n",
        "with open(output_file_path, \"w\") as output_file:\n",
        "    json.dump(cognitive_map_data, output_file, indent=4)\n",
        "\n",
        "print(f\"Final cognitive map saved to: {output_file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45PD1zXP9045",
        "outputId": "c17bc756-7731-4d3c-f748-cf02b2b359a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final cognitive map saved to: /content/final_cognitive_map_sbert.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA *CLEANING*"
      ],
      "metadata": {
        "id": "gmc-i5SYqVSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from os import replace\n",
        "import json\n",
        "import re  # Import regex module\n",
        "\n",
        "# Load final cognitive map\n",
        "with open(\"/content/final_cognitive_map_sbert.json\", \"r\") as f:\n",
        "    cognitive_map_data = json.load(f)\n",
        "\n",
        "# Define standardization rules (common replacements)\n",
        "phrases_to_remove = [\n",
        "    \" (New Context Node)\",\n",
        "    \"- Intersection type:\",\n",
        "    \"Intersections type\",\n",
        "    \"Intersection type\",\n",
        "    \"Intersection type: \",\n",
        "    \"There is not environment description\",\n",
        "    \", environment descriptions:\",\n",
        "    \"- Specific stores restaurant:\",\n",
        "    \"Specific\",\n",
        "    \"\\u2764\\ufe0f \",\n",
        "    \": \\u70e4\\u8089\\u5e97\",\n",
        "    \"with \",\n",
        "    \"- :\",\n",
        "    \"(\",\n",
        "    \")\",\n",
        "    \" s,\",\n",
        "    \"on the windows\",\n",
        "    \"outdoor dining\",\n",
        "    \" and\",\n",
        "\n",
        "]\n",
        "\n",
        "# Regular expression to remove \"(Semantic Match: n)\"\n",
        "semantic_match_pattern = r\"\\Semantic Match: \\d+(\\.\\d+)?\"\n",
        "\n",
        "# Remove unnecessary expressions and format correctly\n",
        "def clean_description(description):\n",
        "    desc_cleaned = description.strip()  # Remove leading/trailing spaces\n",
        "\n",
        "    # Remove predefined phrases\n",
        "    for phrase in phrases_to_remove:\n",
        "        desc_cleaned = desc_cleaned.replace(phrase, \"\").strip()\n",
        "        # Replace \"traffic light\" with \"intersection\"\n",
        "        desc_cleaned = desc_cleaned.replace(\"STOP\", \"Stop-sign\").replace(\"intersection\", \"Intersection\").replace(\"; Matched Scene:\",\",\")\\\n",
        "                                    .replace(\";\",\",\").replace(\", \",\",\").replace(\",\",\", \").replace(\"(Intersection with crosswalk)\" , \"crosswalk\")\\\n",
        "                                    .replace(\"(4-way Intersection)\" , \"4-way Intersection\").replace(\"  \",\" \")\\\n",
        "                                    .replace(\"(T-Intersection)\" , \"T-Intersection\").replace(\"Intersections\" , \"Intersection\")\\\n",
        "                                    .replace(\"Intersection crosswalk\" , \"Intersection, crosswalk\").replace(\"stops\",\"stop\")\\\n",
        "                                    .replace(\"Intersection 4-way Intersection\" , \"Intersection, 4-way Intersection\")\\\n",
        "                                    .replace(\"on buildings\", \"Buildings\").replace(\" (Target)\", \", Target\")\\\n",
        "                                    .replace(\"PARKING\", \"parking\").replace(\"on the wall\", \"sign on the Wall\")\\\n",
        "                                    .replace(\"on awnings\", \"sign on Awnings\").replace(\"Restaurant\", \"restaurant\")\\\n",
        "                                    .replace(\"on building windows\",\"sign on Building windows \").replace(\"TD,\",\"TD Bank,\")\\\n",
        "                                    .replace(\"TD bank\",\"TD Bank\").replace(\"Chase Bank\",\"Chase\").replace(\"Chase bank\",\"Chase\")\\\n",
        "                                    .replace(\"Chase\",\"Chase Bank\").replace(\"Target, Target,\",\"Target\").replace(\"Laight\",\"Light\")\\\n",
        "                                    .replace(\"Starting Point, Starting Point,\",\"Starting Point\").replace(\"LAUNDROMAT\", \"Laundromat\")\\\n",
        "                                    .replace(\"Bike Rental, Bike Rental\",\"Bike Rental\").strip()\n",
        "\n",
        "\n",
        "    # Remove semantic match using regex\n",
        "    desc_cleaned = re.sub(semantic_match_pattern, \"\", desc_cleaned).strip()\n",
        "\n",
        "    return desc_cleaned  # Keep original capitalization\n",
        "\n",
        "# Apply the cleaning function to all nodes\n",
        "for instance in cognitive_map_data:\n",
        "    nodes = instance[\"Graph\"][\"Nodes\"]\n",
        "    for node_id in nodes:\n",
        "        cleaned_description = clean_description(nodes[node_id])\n",
        "        node_description = cleaned_description.title()\n",
        "        # Apply the formatted description back to the node\n",
        "        nodes[node_id] = node_description\n",
        "\n",
        "# Save the refined cognitive map\n",
        "output_path = \"/content/standardized_cognitive_map.json\"\n",
        "with open(output_path, \"w\") as f:\n",
        "    json.dump(cognitive_map_data, f, indent=4)\n",
        "\n",
        "print(f\"Updated cognitive map saved to: {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2c13_pBqDkT",
        "outputId": "c1d31163-6556-40be-bf65-6e0c73478c5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated cognitive map saved to: /content/standardized_cognitive_map.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load final cognitive map\n",
        "file_path = \"/content/standardized_cognitive_map.json\"\n",
        "with open(file_path, \"r\") as f:\n",
        "    cognitive_map_data = json.load(f)\n",
        "\n",
        "# Define intersection priority (higher index = lower priority)\n",
        "intersection_priority = [\n",
        "    \"T-Intersection\",\n",
        "    \"4-Way Intersection\",\n",
        "    \"Intersection\",\n",
        "    \"Traffic Light\"\n",
        "\n",
        "]\n",
        "\n",
        "# Function to refine description while keeping landscape details\n",
        "def refine_description(desc):\n",
        "    parts = [part.strip() for part in desc.split(\",\")]  # Split descriptions by commas and remove extra spaces\n",
        "    highest_priority = None\n",
        "    remaining_landscape = []\n",
        "\n",
        "    # Identify highest-priority intersection type and preserve other landscape details\n",
        "    for part in parts:\n",
        "        if part in intersection_priority:\n",
        "            if highest_priority is None or intersection_priority.index(part) < intersection_priority.index(highest_priority):\n",
        "                highest_priority = part  # Assign the highest-priority intersection type\n",
        "        else:\n",
        "            remaining_landscape.append(part)  # Keep landscape elements\n",
        "\n",
        "    # If no intersection type is found, return the original description unchanged\n",
        "    if highest_priority is None:\n",
        "        return desc\n",
        "\n",
        "    # Reconstruct the final description with priority intersection and landscape elements\n",
        "    final_description = \", \".join(filter(None, [highest_priority] + remaining_landscape)).strip(\", \")\n",
        "    return final_description\n",
        "\n",
        "# Process each instance\n",
        "for instance in cognitive_map_data:\n",
        "    nodes = instance[\"Graph\"][\"Nodes\"]\n",
        "\n",
        "    # Update node descriptions while keeping landscape details\n",
        "    for node_id, desc in nodes.items():\n",
        "        nodes[node_id] = refine_description(desc)\n",
        "\n",
        "# Save the refined cognitive map\n",
        "output_path = \"/content/final_refined_intersections.json\"\n",
        "with open(output_path, \"w\") as f:\n",
        "    json.dump(cognitive_map_data, f, indent=4)\n",
        "\n",
        "print(f\"Updated cognitive map saved to: {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4FD8WAFqKuz",
        "outputId": "4329b7ce-43a7-4262-c713-44d8d2dec511"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated cognitive map saved to: /content/final_refined_intersections.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load the cognitive map data\n",
        "file_path = \"/content/final_refined_intersections.json\"\n",
        "with open(file_path, \"r\") as f:\n",
        "    cognitive_map_data = json.load(f)\n",
        "\n",
        "# Function to remove duplicate words/phrases while maintaining order\n",
        "def remove_duplicates(description):\n",
        "    words = description.split(\", \")\n",
        "    seen = set()\n",
        "    unique_words = [word for word in words if not (word in seen or seen.add(word))]\n",
        "    return \", \".join(unique_words)\n",
        "\n",
        "# Organize nodes based on edges grouping by 'from' node\n",
        "def organize_nodes(instance):\n",
        "    edges = instance[\"Graph\"][\"Edges\"]\n",
        "    nodes = instance[\"Graph\"][\"Nodes\"]\n",
        "\n",
        "    # Group edges by 'from' node\n",
        "    grouped_edges = {}\n",
        "    for edge in edges:\n",
        "        from_node = edge[\"from\"]\n",
        "        if from_node not in grouped_edges:\n",
        "            grouped_edges[from_node] = []\n",
        "        grouped_edges[from_node].append(edge)\n",
        "\n",
        "    # Reconstruct ordered nodes dictionary with sequential numbering\n",
        "    ordered_nodes = {}\n",
        "    sequence_mapping = {}\n",
        "    sequence = 1\n",
        "    visited_nodes = set()\n",
        "\n",
        "    for from_node in sorted(nodes.keys(), key=int):\n",
        "        if from_node not in visited_nodes:\n",
        "            sequence_mapping[from_node] = sequence\n",
        "            ordered_nodes[sequence] = remove_duplicates(nodes[from_node])\n",
        "            visited_nodes.add(from_node)\n",
        "            sequence += 1\n",
        "\n",
        "        if from_node in grouped_edges:\n",
        "            for edge in grouped_edges[from_node]:\n",
        "                to_node = edge[\"to\"]\n",
        "                if to_node not in visited_nodes and to_node in nodes:\n",
        "                    sequence_mapping[to_node] = sequence\n",
        "                    ordered_nodes[sequence] = remove_duplicates(nodes[to_node])\n",
        "                    visited_nodes.add(to_node)\n",
        "                    sequence += 1\n",
        "\n",
        "    # Update edges with new sequence numbers and group by 'from'\n",
        "    updated_edges = {}\n",
        "    for edge in edges:\n",
        "        new_from = sequence_mapping.get(edge[\"from\"], edge[\"from\"])\n",
        "        new_to = sequence_mapping.get(edge[\"to\"], edge[\"to\"])\n",
        "        if new_from not in updated_edges:\n",
        "            updated_edges[new_from] = []\n",
        "        updated_edges[new_from].append({\"from\": new_from, \"to\": new_to, \"action\": edge[\"action\"]})\n",
        "\n",
        "    instance[\"Graph\"][\"Nodes\"] = ordered_nodes\n",
        "    instance[\"Graph\"][\"Edges\"] = [edge for edges_list in updated_edges.values() for edge in edges_list]\n",
        "\n",
        "# Process each instance and clean node descriptions\n",
        "for instance in cognitive_map_data:\n",
        "    organize_nodes(instance)\n",
        "\n",
        "# Save the cleaned cognitive map\n",
        "output_path = \"/content/cleaned_cognitive_map_cost.json\"\n",
        "with open(output_path, \"w\") as f:\n",
        "    json.dump(cognitive_map_data, f, indent=4)\n",
        "\n",
        "print(f\"Updated cognitive map saved to: {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFgpJUW_qOtg",
        "outputId": "f182fe87-e43d-4435-a7c8-e5ae8c5234cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated cognitive map saved to: /content/cleaned_cognitive_map_cost.json\n"
          ]
        }
      ]
    }
  ]
}
# Cognitive Map-Based Vision-Language Navigation
Map-from-Directions: Vision-Language Navigation with Cognitive Map Generated from Natural Language Instructions via Multimodal Scene Alignment

# Cognitive Map-Based Vision-Language Navigation

This repository contains the implementation for the AAAI-26 submission titled:  
**"Cognitive Map Generation from Natural Language Instructions via Multimodal Scene Alignment"**

It includes notebooks for cognitive map construction, visual environment interpretation, alignment, and evaluation using the Touchdown and Map2Seq datasets.

---

## ğŸ“ Repository Structure

```
.
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ OK_LMM_V3_GPT_4o_Map2seq.ipynb
â”‚   â”œâ”€â”€ OK_Images_environment_Map2seq.ipynb
â”‚   â”œâ”€â”€ OK_environment_description.ipynb
â”‚   â”œâ”€â”€ OK_clean_data_description_env.ipynb
â”‚   â”œâ”€â”€ OK_matching__inc_sem_Dynamic_programing.ipynb
â”‚   â”œâ”€â”€ OK_Alignment.ipynb
â”‚   â””â”€â”€ OK_Gold_path_evaluation_map2seq.ipynb
â”œâ”€â”€ data/
â”œâ”€â”€ outputs/
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ§° Requirements

Install dependencies with:

```bash
pip install -r requirements.txt
```

Key packages:
- `openai`
- `sentence-transformers`
- `networkx`
- `matplotlib`
- `pandas`
- `opencv-python`
- `numpy`

---

## ğŸ§ª Reproducing the Results

### ğŸ“Š Pipeline Summary: Input â†’ Run â†’ Output

| Step | Notebook | Input | Run | Output |
|------|----------|-------|-----|--------|
| 1ï¸âƒ£ | `OK_LMM_V3_GPT_4o_Map2seq.ipynb` | Navigation instructions (`*.json` from Map2Seq/Touchdown) | GPT-4o parses instructions to generate graph | `dev_graph_lmm.json` (Cognitive Map) |
| 2ï¸âƒ£ | `OK_Images_environment_Map2seq.ipynb` | Waypoints (lat/lon), headings | Fetch and stitch panoramic images | Folder with stitched GSV images |
| 3ï¸âƒ£ | `OK_environment_description.ipynb` | GSV images (from Step 2) | GPT-4 Vision generates scene descriptions | `raw_environment_descriptions.json` |
| 3.5ï¸âƒ£ | `OK_clean_data_description_env.ipynb` | Raw scene descriptions (`raw_environment_descriptions.json`) | Clean and normalize descriptions | `cleaned_environment_descriptions.json` |
| 4ï¸âƒ£ | `OK_matching__inc_sem_Dynamic_programing.ipynb` | Cognitive Map + Cleaned Visual Descriptions | Align via SBERT and Dynamic Programming | `matched_nodes.json`, `alignment_scores.json` |
| 5ï¸âƒ£ | `OK_Alignment.ipynb` | Cognitive Map + Visual Alignments | Merge aligned nodes into final graph | `final_matched_graph.json` |
| 6ï¸âƒ£ | `OK_Gold_path_evaluation_map2seq.ipynb` | Predicted paths vs. gold paths | Evaluate navigation success | TC, KPA, SPD scores + visual plots |

---

## ğŸ“ Notes

- You need an **OpenAI API key** to access GPT-4o and GPT-4 Vision capabilities.
- Ensure image input/output paths are consistent across notebooks.
- The system was tested using Python 3.10 and a single GPU.

---

## ğŸ“œ License

MIT License
